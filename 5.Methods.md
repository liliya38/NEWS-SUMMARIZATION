# Methods
Here I have used three different models which summarizes the news. They are:-
## Bert Summarizing
It is a pre-trained model that is naturally bidirectional. This pre-trained model can be tuned to easily to perform the NLP tasks as specified, Summarization in our case.
## GPT-2 Summarizing
GPT-2 was trained with the goal of causal language modeling (CLM) and is thus capable of predicting the next token in a sequence. GPT-2 may create syntactically coherent text by utilizing this capability. GPT2 is capable of several tasks, including summarization, generation, and translation.
## XLNet Summarizing
XLNet is a generalized autoregressive language model that learns unsupervised representations of text sequences. This model incorporates modelling techniques from Autoencoder(AE) models(BERT) into AR models while avoiding limitations of AE.
